{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datasets con datos de partida limpia\n",
    "df_2024 = pd.read_csv('datos__2024.csv', parse_dates=['Timestamp'], sep=';')\n",
    "df_2025 = pd.read_csv('datos__2025.csv', parse_dates=['Timestamp'], sep=';')\n",
    "df_2025_last = pd.read_csv('datos__2025_last.csv', parse_dates=['Timestamp'])\n",
    "df_actual = pd.read_csv(r\"C:\\Users\\nicob\\Downloads\\datos_nuevos.csv\", parse_dates=['Timestamp'])\n",
    "\n",
    "# Convertir columna 'Timestamp' a tipo datetime para eliminar la informaci√≥n de zona horaria (tz)\n",
    "df_2024['Timestamp'] = pd.to_datetime(df_2024['Timestamp'])\n",
    "df_2024['Timestamp'] = df_2024['Timestamp'].dt.tz_localize(None)\n",
    "df_2025['Timestamp'] = pd.to_datetime(df_2025['Timestamp'])\n",
    "df_2025['Timestamp'] = df_2025['Timestamp'].dt.tz_localize(None)\n",
    "df_2025_last['Timestamp'] = pd.to_datetime(df_2025_last['Timestamp'])\n",
    "df_2025_last['Timestamp'] = df_2025_last['Timestamp'].dt.tz_localize(None)\n",
    "df_actual['Timestamp'] = pd.to_datetime(df_actual['Timestamp'])\n",
    "df_actual['Timestamp'] = df_actual['Timestamp'].dt.tz_localize(None)\n",
    "\n",
    "# Unir y eliminar registros duplicados entre los datasets\n",
    "df = pd.concat([df_2024, df_2025, df_2025_last, df_actual])\n",
    "df = df.drop_duplicates(subset=['Timestamp'], keep='last')\n",
    "df.sort_values('Timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar datos para generar archivo adicional de testing final\n",
    "fecha_test_final = pd.to_datetime('2025-04-01')\n",
    "df_test_final = df[df['Timestamp'] > fecha_test_final].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar datos de entrenamiento de diciembre en adelante (Partida Limpia)\n",
    "fecha_inicio = pd.to_datetime('2024-12-20')\n",
    "df = df[(df['Timestamp'] >= fecha_inicio) & (df['Timestamp'] <= fecha_test_final)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('datos_diciembre_marzo_entrenamiento.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creacion de columnas auxiliares para filtrado de datos operacionales\n",
    "df['RUN_SHIFT'] = df['CNN-3200-CR_0001_MO.RUN'].shift(1, fill_value=0)  # Estado anterior de RUN del motor\n",
    "df['CAMBIO_RUN'] = (df['RUN_SHIFT'] == 0) & (df['CNN-3200-CR_0001_MO.RUN'] == 1)  # Detectar cambios de 0 a 1 en el RUN del motor\n",
    "\n",
    "# Crear una columna de omisi√≥n para los primeros 8 registros despu√©s del cambio\n",
    "df['OMITIR'] = df['CAMBIO_RUN'].rolling(window=8, min_periods=1).max()\n",
    "\n",
    "# Filtrar dataset con los datos operativos (motor en funcionamiento, presencia de material de chancado y omisi√≥n de datos de partida)\n",
    "df = df[(df['CNN-3200-CR_0001_MO.RUN'] == 1) & (df['OMITIR'] != 1) & (df['CNN-3200-WIC32149.PV'] > 700)].drop(columns=['RUN_SHIFT', 'CAMBIO_RUN', 'OMITIR'])\n",
    "\n",
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df.columns.difference(['Timestamp', 'archivo_origen']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar columnas utiles para el modelo\n",
    "columnas = [\n",
    "    'Timestamp',\n",
    "    'CNN-3200-CR_0001_MO.PWR',\n",
    "    'CNN-3200-CR_0001_MO.CUR',\n",
    "    'CNN-3200-FIT32053.PV',\n",
    "    'CNN-3200-FIT32054.PV',\n",
    "    'CNN-3200-PIT32031.PV',\n",
    "    'CNN-3200-PIT32043.PV',\n",
    "    'CNN-3200-PIT32056.PV',\n",
    "    'CNN-3200-TIT32045.PV',\n",
    "    'CNN-3200-TIT32046.PV'\n",
    "]\n",
    "\n",
    "df = df[columnas]\n",
    "\n",
    "# Rellenar valores faltantes\n",
    "df = df.interpolate(method='linear', limit_direction='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el dataset preprocesado\n",
    "df.to_csv('datos_procesados_vf.csv', index=False)\n",
    "\n",
    "# Guardar dataset para testeo final\n",
    "df_test_final.to_csv('data_abril_test_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_sensores = [\n",
    "    'CNN-3200-CR_0001_MO.PWR',\n",
    "    'CNN-3200-CR_0001_MO.CUR',\n",
    "    'CNN-3200-FIT32053.PV',\n",
    "    'CNN-3200-FIT32054.PV',\n",
    "    'CNN-3200-PIT32031.PV',\n",
    "    'CNN-3200-PIT32043.PV',\n",
    "    'CNN-3200-PIT32056.PV',\n",
    "    'CNN-3200-TIT32045.PV',\n",
    "    'CNN-3200-TIT32046.PV'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"deep\") # Paleta de colores\n",
    "for sensor in cols:\n",
    "    df_2[sensor] = df_2[sensor].round(2)\n",
    "    min_val = df_2[sensor].min()\n",
    "    max_val = df_2[sensor].max()\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    sns.histplot(df_2[sensor], bins=100, stat='density', color=[0.8]*3)\n",
    "    sns.kdeplot(df_2[sensor], color=palette[3])\n",
    "    etiquetas = np.round(np.linspace(min_val, max_val, 27), 2)\n",
    "    plt.xticks(etiquetas)\n",
    "    plt.title(f'Distribuci√≥n de {sensor} durante operaci√≥n')\n",
    "    plt.xlabel(sensor)\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"deep\") # Paleta de colores\n",
    "for sensor in columnas_sensores:\n",
    "    min_val = df[sensor].min()\n",
    "    max_val = df[sensor].max()\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    sns.histplot(df[sensor], bins=100, stat='density', color=[0.8]*3)\n",
    "    sns.kdeplot(df[sensor], color=palette[3])\n",
    "    plt.xticks(np.linspace(min_val, max_val, 27))\n",
    "    plt.title(f'Distribuci√≥n de {sensor} durante operaci√≥n')\n",
    "    plt.xlabel(sensor)\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la desviaci√≥n est√°ndar en ventanas de 2 minuto (8 registros)\n",
    "df_2_std = df_2.copy()\n",
    "for sensor in cols:\n",
    "    df_2_std[f'{sensor}_std'] = df_2[sensor].rolling(window=8, min_periods=1).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar histogramas de las desviaciones est√°ndar\n",
    "palette = sns.color_palette(\"deep\")\n",
    "for sensor in cols:\n",
    "    min_val = df_2_std[f'{sensor}_std'].min()\n",
    "    max_val = df_2_std[f'{sensor}_std'].max()\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    sns.histplot(df_2_std[f'{sensor}_std'], bins=100, stat='density', color=[0.8]*3)\n",
    "    sns.kdeplot(df_2_std[f'{sensor}_std'], color=palette[3])\n",
    "    plt.xticks(np.linspace(min_val, max_val, 27))\n",
    "    plt.title(f'Distribuci√≥n de la desviaci√≥n est√°ndar de {sensor} en ventanas de 1 minuto')\n",
    "    plt.xlabel(f'{sensor} (Desviaci√≥n est√°ndar)')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la desviaci√≥n est√°ndar en ventanas de 2 minuto (8 registros)\n",
    "df_std = df.copy()\n",
    "for sensor in columnas_sensores:\n",
    "    df_std[f'{sensor}_std'] = df[sensor].rolling(window=8, min_periods=1).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar histogramas de las desviaciones est√°ndar\n",
    "palette = sns.color_palette(\"deep\")\n",
    "for sensor in columnas_sensores:\n",
    "    min_val = df_std[f'{sensor}_std'].min()\n",
    "    max_val = df_std[f'{sensor}_std'].max()\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    sns.histplot(df_std[f'{sensor}_std'], bins=100, stat='density', color=[0.8]*3)\n",
    "    sns.kdeplot(df_std[f'{sensor}_std'], color=palette[3])\n",
    "    plt.xticks(np.linspace(min_val, max_val, 27))\n",
    "    plt.title(f'Distribuci√≥n de la desviaci√≥n est√°ndar de {sensor} en ventanas de 1 minuto')\n",
    "    plt.xlabel(f'{sensor} (Desviaci√≥n est√°ndar)')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std = df_std.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limites_sensores = {\n",
    "    'CNN-3200-CR_0001_MO.PWR': {\"min\": 0, \"max\": 590},\n",
    "    'CNN-3200-CR_0001_MO.CUR': {\"min\": 42, \"max\": 100},\n",
    "    'CNN-3200-FIT32053.PV': {\"min\": 60, \"max\": 92},\n",
    "    'CNN-3200-FIT32054.PV': {\"min\": 106, \"max\": 115},\n",
    "    'CNN-3200-PIT32031.PV': {\"min\": 120, \"max\": 265.5},\n",
    "    'CNN-3200-PIT32043.PV': {\"min\": 77, \"max\": 131},\n",
    "    'CNN-3200-PIT32056.PV': {\"min\": 55, \"max\": 148},\n",
    "    'CNN-3200-TIT32045.PV': {\"min\": 39.7, \"max\": 47.7},\n",
    "    'CNN-3200-TIT32046.PV': {\"min\": 39.8, \"max\": 46.5}\n",
    "}\n",
    "\n",
    "limites_std = {\n",
    "    'CNN-3200-CR_0001_MO.PWR_std': {\"max\": 224},\n",
    "    'CNN-3200-CR_0001_MO.CUR_std': {\"max\": 26},\n",
    "    'CNN-3200-FIT32053.PV_std': {\"max\": 1.5},\n",
    "    'CNN-3200-FIT32054.PV_std': {\"max\": 0.9},\n",
    "    'CNN-3200-PIT32031.PV_std': {\"max\": 61.5},\n",
    "    'CNN-3200-PIT32043.PV_std': {\"max\": 2.7},\n",
    "    'CNN-3200-PIT32056.PV_std': {\"max\": 18.5},\n",
    "    'CNN-3200-TIT32045.PV_std': {\"max\": 0.85},\n",
    "    'CNN-3200-TIT32046.PV_std': {\"max\": 0.81}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para determinar si un registro es posible falla\n",
    "def detectar_falla(row):\n",
    "    for sensor, limites in limites_sensores.items():\n",
    "        if row[sensor] < limites[\"min\"] or row[sensor] > limites[\"max\"]:\n",
    "            return 1  # Posible falla\n",
    "    \n",
    "    for sensor_std, limites in limites_std.items():\n",
    "        if row[sensor_std] > limites[\"max\"]:\n",
    "            return 1  # Posible falla\n",
    "    \n",
    "    return 0  # Normal\n",
    "\n",
    "# Aplicar funci√≥n a cada fila\n",
    "df_std[\"Posible_Falla\"] = df_std.apply(detectar_falla, axis=1)\n",
    "\n",
    "# Guardar dataset etiquetado\n",
    "df_std.to_csv(\"datos_etiquetados_vf.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Seleccionar registros normales (sin fallas)\n",
    "df_normal = df_std[df_std[\"Posible_Falla\"] == 0].copy()\n",
    "\n",
    "# N√∫mero de fallas a generar (60% del dataset normal)\n",
    "num_fallas = int(len(df_normal) * 0.60)\n",
    "\n",
    "# Dividir en tres tipos de fallas sint√©ticas\n",
    "num_fallas_1 = int(num_fallas * 0.34)  # Alteraciones en sensor y std\n",
    "num_fallas_2 = int(num_fallas * 0.33)  # Alteraciones solo en el sensor\n",
    "num_fallas_3 = num_fallas - num_fallas_1 - num_fallas_2  # Alteraciones solo en la std\n",
    "\n",
    "# Seleccionar registros aleatorios para cada tipo\n",
    "fallas_1 = df_normal.sample(n=num_fallas_1, random_state=42).copy()\n",
    "fallas_2 = df_normal.sample(n=num_fallas_2, random_state=43).copy()\n",
    "fallas_3 = df_normal.sample(n=num_fallas_3, random_state=44).copy()\n",
    "\n",
    "# Aplicar modificaciones en los sensores\n",
    "for sensor, limites in limites_sensores.items():\n",
    "    desviacion = (limites[\"max\"] - limites[\"min\"]) * 0.4  # Anomal√≠a del 40% del rango normal\n",
    "\n",
    "    # Tipo 1: Alteraciones en el sensor y std\n",
    "    fallas_1[sensor] = np.where(\n",
    "        np.random.rand(len(fallas_1)) > 0.5,\n",
    "        fallas_1[sensor] + desviacion,  # Aumentar el valor\n",
    "        fallas_1[sensor] - desviacion   # Disminuir el valor\n",
    "    )\n",
    "\n",
    "    # Tipo 2: Alteraciones solo en el sensor (manteniendo std normal)\n",
    "    fallas_2[sensor] = np.where(\n",
    "        np.random.rand(len(fallas_2)) > 0.5,\n",
    "        fallas_2[sensor] + desviacion,  \n",
    "        fallas_2[sensor] - desviacion\n",
    "    )\n",
    "\n",
    "# Aplicar modificaciones en las desviaciones est√°ndar\n",
    "for sensor_std, limites in limites_std.items():\n",
    "    aumento_std = limites[\"max\"] * 1.7  # Aumentar 70% sobre el l√≠mite m√°ximo\n",
    "\n",
    "    # Tipo 1: Alteraciones en el sensor y std\n",
    "    fallas_1[sensor_std] += aumento_std  \n",
    "\n",
    "    # Tipo 3: Alteraciones solo en std (manteniendo valores normales)\n",
    "    fallas_3[sensor_std] += aumento_std  \n",
    "\n",
    "# Etiquetar los registros como fallas\n",
    "fallas_1[\"Posible_Falla\"] = 1\n",
    "fallas_2[\"Posible_Falla\"] = 1\n",
    "fallas_3[\"Posible_Falla\"] = 1\n",
    "\n",
    "# Combinar dataset original con fallas sint√©ticas\n",
    "df_final = pd.concat([df_std, fallas_1, fallas_2, fallas_3]).sort_values(by=\"Timestamp\").reset_index(drop=True)\n",
    "\n",
    "# Guardar dataset con fallas sint√©ticas\n",
    "df_final.to_csv(\"datos_con_fallas_sinteticas_vf.csv\", index=False)\n",
    "\n",
    "print(f\"‚úÖ Se generaron {num_fallas} fallas sint√©ticas y se guardaron en 'datos_con_fallas_sinteticas_vf.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar registros normales y con fallas\n",
    "print(df_final[\"Posible_Falla\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor in columnas_sensores:\n",
    "    plt.figure(figsize=(20, 4))\n",
    "\n",
    "    # Histograma del sensor en operaci√≥n normal\n",
    "    sns.histplot(df_final[df_final[\"Posible_Falla\"] == 0][sensor], bins=80, color=\"blue\", label=\"Normal\", stat=\"density\", alpha=0.5)\n",
    "\n",
    "    # Histograma del sensor en registros de falla\n",
    "    sns.histplot(df_final[df_final[\"Posible_Falla\"] == 1][sensor], bins=80, color=\"red\", label=\"Falla\", stat=\"density\", alpha=0.5)\n",
    "\n",
    "    plt.title(f'Comparaci√≥n de distribuci√≥n - {sensor}')\n",
    "    plt.xlabel(sensor)\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver cu√°ntos registros de fallas est√°n fuera de los l√≠mites\n",
    "for sensor, limites in limites_sensores.items():\n",
    "    fuera_limite = df_final[(df_final[\"Posible_Falla\"] == 1) & \n",
    "                      ((df_final[sensor] < limites[\"min\"]) | (df_final[sensor] > limites[\"max\"]))]\n",
    "    print(f\"{sensor}: {len(fuera_limite)} registros de fallas fuera de los l√≠mites esperados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor_std in limites_std.keys():\n",
    "    print(f\"{sensor_std}:\")\n",
    "    print(\"  Media en operaci√≥n normal:\", df_final[df_final[\"Posible_Falla\"] == 0][sensor_std].mean())\n",
    "    print(\"  Media en fallas:\", df_final[df_final[\"Posible_Falla\"] == 1][sensor_std].mean(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos con fallas sint√©ticas\n",
    "df = pd.read_csv(\"datos_con_fallas_sinteticas_2.csv\", parse_dates=['Timestamp'])\n",
    "\n",
    "# Ordenar por Timestamp por seguridad\n",
    "df = df.sort_values(by=\"Timestamp\").reset_index(drop=True)\n",
    "\n",
    "# Definir columnas de entrada (excluyendo Timestamp y Posible_Falla)\n",
    "columnas_sensores = [col for col in df.columns if col not in ['Timestamp', 'Posible_Falla']]\n",
    "\n",
    "# Normalizaci√≥n (MinMaxScaler entre 0 y 1)\n",
    "scaler = MinMaxScaler()\n",
    "df[columnas_sensores] = scaler.fit_transform(df[columnas_sensores])\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "# Definir ventana de tiempo para LSTM (ej: 10 registros = 2.5 minutos de contexto)\n",
    "ventana = 10\n",
    "\n",
    "# Crear secuencias de datos para LSTM\n",
    "X, y = [], []\n",
    "for i in range(len(df) - ventana):\n",
    "    X.append(df[columnas_sensores].iloc[i:i+ventana].values)  # 10 registros anteriores\n",
    "    y.append(df[\"Posible_Falla\"].iloc[i+ventana])  # Predicci√≥n para el siguiente registro\n",
    "\n",
    "# Convertir a arrays de NumPy\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Dividir en conjunto de entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"‚úÖ Datos listos para el modelo LSTM:\")\n",
    "print(f\"  - X_train shape: {X_train.shape}\")  # (num_samples, ventana, num_features)\n",
    "print(f\"  - X_test shape: {X_test.shape}\")\n",
    "print(f\"  - y_train shape: {y_train.shape}\")\n",
    "print(f\"  - y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Definir modelo LSTM\n",
    "modelo = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(10, 18)),  # Capa LSTM con 64 neuronas\n",
    "    Dropout(0.2),  # Regularizaci√≥n\n",
    "    LSTM(32, return_sequences=False),  # Segunda capa LSTM\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),  # Capa densa\n",
    "    Dense(1, activation='sigmoid')  # Salida con probabilidad de falla\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "modelo.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Resumen de la arquitectura\n",
    "modelo.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Definir un callback para detener el entrenamiento si la validaci√≥n no mejora\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = modelo.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,  # N√∫mero de √©pocas (ajustable)\n",
    "    batch_size=64,  # Tama√±o de lote (ajustable)\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "modelo.save(\"modelo_lstm_fallas.h5\")\n",
    "\n",
    "print(\"‚úÖ Entrenamiento completado y modelo guardado como 'modelo_lstm_fallas.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(modelo, \"modelo_lstm_fallas.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = (modelo.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Mostrar la matriz de confusi√≥n\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Falla\"], yticklabels=[\"Normal\", \"Falla\"])\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusi√≥n\")\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificaci√≥n\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Normal\", \"Falla\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el umbral a 0.3 en vez de 0.5\n",
    "umbral = 0.3\n",
    "y_pred_ajustado = (modelo.predict(X_test) > umbral).astype(\"int32\")\n",
    "\n",
    "# Nueva matriz de confusi√≥n\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_ajustado)\n",
    "\n",
    "# Mostrar la nueva matriz de confusi√≥n\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Falla\"], yticklabels=[\"Normal\", \"Falla\"])\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(f\"Matriz de Confusi√≥n con umbral {umbral}\")\n",
    "plt.show()\n",
    "\n",
    "# Nuevo reporte de clasificaci√≥n\n",
    "print(classification_report(y_test, y_pred_ajustado, target_names=[\"Normal\", \"Falla\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Definir el modelo CNN + LSTM\n",
    "modelo_cnn_lstm = Sequential([\n",
    "    # Capa CNN para extraer caracter√≠sticas espaciales\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(10, 18)),\n",
    "    MaxPooling1D(pool_size=2),  # Reduce la dimensionalidad\n",
    "\n",
    "    # Capa LSTM para capturar patrones temporales\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # Capas densas\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Salida con probabilidad de falla\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "modelo_cnn_lstm.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Mostrar resumen del modelo\n",
    "modelo_cnn_lstm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Definir Early Stopping para evitar sobreentrenamiento\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Entrenar el modelo CNN + LSTM\n",
    "history = modelo_cnn_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,  # N√∫mero de √©pocas (ajustable)\n",
    "    batch_size=256,  # Tama√±o de lote\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "modelo_cnn_lstm.save(\"modelo_cnn_lstm_fallas.h5\")\n",
    "\n",
    "print(\"‚úÖ Entrenamiento completado y modelo guardado como 'modelo_cnn_lstm_fallas.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predecir en el conjunto de prueba con el umbral original (0.5)\n",
    "y_pred_cnn_lstm = (modelo_cnn_lstm.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_cnn_lstm)\n",
    "\n",
    "# Mostrar la matriz de confusi√≥n\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Falla\"], yticklabels=[\"Normal\", \"Falla\"])\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusi√≥n - Modelo CNN + LSTM\")\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificaci√≥n\n",
    "print(classification_report(y_test, y_pred_cnn_lstm, target_names=[\"Normal\", \"Falla\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar umbral de clasificaci√≥n a 0.3\n",
    "umbral = 0.3\n",
    "y_pred_cnn_lstm_ajustado = (modelo_cnn_lstm.predict(X_test) > umbral).astype(\"int32\")\n",
    "\n",
    "# Matriz de confusi√≥n con nuevo umbral\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_cnn_lstm_ajustado)\n",
    "\n",
    "# Mostrar la matriz de confusi√≥n\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Falla\"], yticklabels=[\"Normal\", \"Falla\"])\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(f\"Matriz de Confusi√≥n - CNN + LSTM (Umbral {umbral})\")\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificaci√≥n\n",
    "print(classification_report(y_test, y_pred_cnn_lstm_ajustado, target_names=[\"Normal\", \"Falla\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Bidirectional, Dense, Dropout, BatchNormalization\n",
    "\n",
    "# Definir el modelo optimizado CNN + BiLSTM\n",
    "modelo_opt = Sequential([\n",
    "    # Primera capa CNN\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(10, 18)),\n",
    "    BatchNormalization(),  # Normalizaci√≥n para estabilidad\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Segunda capa CNN para mayor profundidad\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Capa LSTM Bidireccional\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Segunda capa LSTM Bidireccional\n",
    "    Bidirectional(LSTM(32, return_sequences=False)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Capas densas\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Salida con probabilidad de falla\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "modelo_opt.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "                   loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Mostrar el resumen del modelo\n",
    "modelo_opt.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Definir Early Stopping para evitar sobreentrenamiento\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Entrenar el modelo optimizado\n",
    "history_opt = modelo_opt.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,  # N√∫mero de √©pocas (ajustable)\n",
    "    batch_size=256,  # Tama√±o de lote\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "modelo_opt.save(\"modelo_optimizado_cnn_bilstm.h5\")\n",
    "\n",
    "print(\"‚úÖ Entrenamiento completado y modelo guardado como 'modelo_optimizado_cnn_bilstm.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predecir en el conjunto de prueba con el umbral original (0.5)\n",
    "y_pred_opt = (modelo_opt.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_opt)\n",
    "\n",
    "# Mostrar la matriz de confusi√≥n\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Falla\"], yticklabels=[\"Normal\", \"Falla\"])\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusi√≥n - Modelo Optimizado CNN + BiLSTM\")\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificaci√≥n\n",
    "print(classification_report(y_test, y_pred_opt, target_names=[\"Normal\", \"Falla\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Bidirectional, Dense, Dropout, BatchNormalization\n",
    "\n",
    "# Definir el modelo optimizado con m√°s capas LSTM\n",
    "modelo_optimizado = Sequential([\n",
    "    # Primera capa CNN con m√°s filtros\n",
    "    Conv1D(filters=256, kernel_size=3, activation='relu', padding='same', input_shape=(10, 18)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Segunda capa CNN con menos filtros\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Primera capa BiLSTM\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Segunda capa BiLSTM\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Tercera capa LSTM tradicional\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Capas densas\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Salida con probabilidad de falla\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "modelo_optimizado.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "                          loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Mostrar el resumen del modelo\n",
    "modelo_optimizado.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Definir Early Stopping para evitar sobreentrenamiento\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Entrenar el modelo mejorado\n",
    "history_mejorado = modelo_optimizado.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,  # N√∫mero de √©pocas\n",
    "    batch_size=256,  # Tama√±o de lote\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "modelo_optimizado.save(\"modelo_mejorado_cnn_bilstm.h5\")\n",
    "\n",
    "print(\"‚úÖ Entrenamiento completado y modelo guardado como 'modelo_mejorado_cnn_bilstm.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predecir en el conjunto de prueba con el umbral original (0.5)\n",
    "y_pred_mejorado = (modelo_optimizado.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_mejorado)\n",
    "\n",
    "# Mostrar la matriz de confusi√≥n\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Falla\"], yticklabels=[\"Normal\", \"Falla\"])\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusi√≥n - Modelo Mejorado CNN + BiLSTM\")\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificaci√≥n\n",
    "print(classification_report(y_test, y_pred_mejorado, target_names=[\"Normal\", \"Falla\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Definir Focal Loss con menor gamma\n",
    "import tensorflow.keras.backend as K\n",
    "def focal_loss(gamma=1.5, alpha=0.25):\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "        weight = alpha * y_true * K.pow(1 - y_pred, gamma)\n",
    "        loss = weight * cross_entropy\n",
    "        return K.mean(loss)\n",
    "    return loss\n",
    "\n",
    "# Definir el modelo LSTM corregido\n",
    "modelo_lstm_corr = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(10, 18)),\n",
    "    LayerNormalization(),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LayerNormalization(),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    LSTM(32, return_sequences=False),\n",
    "    LayerNormalization(),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Salida con probabilidad de falla\n",
    "])\n",
    "\n",
    "# Compilar el modelo con la nueva Focal Loss\n",
    "modelo_lstm_corr.compile(optimizer=Adam(learning_rate=0.0005),\n",
    "                         loss=focal_loss(gamma=1.5, alpha=0.25),\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "# Mostrar el resumen del modelo corregido\n",
    "modelo_lstm_corr.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Definir Early Stopping para evitar sobreentrenamiento\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Entrenar el modelo corregido\n",
    "history_lstm_corr = modelo_lstm_corr.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,  # N√∫mero de √©pocas\n",
    "    batch_size=256,  # Tama√±o de lote\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "modelo_lstm_corr.save(\"modelo_lstm_corregido.h5\")\n",
    "\n",
    "print(\"‚úÖ Entrenamiento completado y modelo guardado como 'modelo_lstm_corregido.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Definir el modelo basado en el art√≠culo\n",
    "modelo_lstm_articulo = Sequential([\n",
    "    LSTM(100, activation='relu', return_sequences=True, input_shape=(10, 18)),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    LSTM(50, activation='relu', return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    LSTM(25, activation='relu', return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(1, activation='sigmoid')  # Salida con probabilidad de falla\n",
    "])\n",
    "\n",
    "# Compilar el modelo con funci√≥n de p√©rdida 'binary_crossentropy'\n",
    "modelo_lstm_articulo.compile(optimizer=Adam(learning_rate=0.0005),\n",
    "                             loss='binary_crossentropy',\n",
    "                             metrics=['accuracy'])\n",
    "\n",
    "# Mostrar el resumen del modelo\n",
    "modelo_lstm_articulo.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir Early Stopping para evitar sobreajuste\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Entrenar el modelo\n",
    "history_lstm_articulo = modelo_lstm_articulo.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,  # N√∫mero de √©pocas\n",
    "    batch_size=64,  # Tama√±o de lote reducido\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "modelo_lstm_articulo.save(\"modelo_lstm_articulo.h5\")\n",
    "\n",
    "print(\"‚úÖ Entrenamiento completado y modelo guardado como 'modelo_lstm_articulo.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Predicciones\n",
    "y_pred_prob = modelo_lstm_articulo.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Umbral de 0.5 para clasificar\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Mostrar matriz de confusi√≥n\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Falla\"], yticklabels=[\"Normal\", \"Falla\"])\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusi√≥n - Modelo LSTM Adaptado\")\n",
    "plt.show()\n",
    "\n",
    "# Reporte de m√©tricas\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el umbral de clasificaci√≥n\n",
    "umbral = 0.3\n",
    "y_pred_ajustado = (y_pred_prob > umbral).astype(int)\n",
    "\n",
    "# Nueva matriz de confusi√≥n\n",
    "cm_ajustado = confusion_matrix(y_test, y_pred_ajustado)\n",
    "\n",
    "# Graficar la nueva matriz de confusi√≥n\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_ajustado, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Falla\"], yticklabels=[\"Normal\", \"Falla\"])\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(f\"Matriz de Confusi√≥n - Umbral {umbral}\")\n",
    "plt.show()\n",
    "\n",
    "# Reporte de m√©tricas\n",
    "print(classification_report(y_test, y_pred_ajustado))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el DataFrame con datos de diciembre\n",
    "df_nuevos = df_2024.copy()\n",
    "fecha_inicio = pd.to_datetime('2024-12-01')\n",
    "df_nuevos = df_nuevos[df_nuevos['Timestamp'] < fecha_inicio].reset_index(drop=True)\n",
    "\n",
    "# Filtrar solo cuando el chancador est√° en RUN = 1\n",
    "df_nuevos = df_nuevos[df_nuevos[\"CNN-3200-CR_0001_MO.RUN\"] == 1]\n",
    "\n",
    "# Omitir los primeros 8 registros tras cambio de estado RUN 0 ‚Üí 1\n",
    "df_nuevos[\"RUN_shift\"] = df_nuevos[\"CNN-3200-CR_0001_MO.RUN\"].shift(1).fillna(0)\n",
    "df_nuevos[\"RUN_cambio\"] = (df_nuevos[\"CNN-3200-CR_0001_MO.RUN\"] == 1) & (df_nuevos[\"RUN_shift\"] == 0)\n",
    "df_nuevos[\"omit\"] = df_nuevos[\"RUN_cambio\"].rolling(window=8, min_periods=1).max()\n",
    "df_nuevos = df_nuevos[(df_nuevos[\"CNN-3200-CR_0001_MO.RUN\"] == 1) & \n",
    "                       (df_nuevos['omit'] != 1) & \n",
    "                       (df_nuevos['CNN-3200-WIC32149.PV'] > 150)]\n",
    "\n",
    "# Eliminar columnas auxiliares\n",
    "df_nuevos.drop(columns=[\"RUN_shift\", \"RUN_cambio\", \"omit\"], inplace=True)\n",
    "\n",
    "# Generar valores sint√©ticos para el sensor faltante\n",
    "rango = [110, 110.3, 111, 110.5, 112, 110.2, 113, 110.8, 112, 114, 112.6, 115, 114.2, 111, 110, 110.11, 114]\n",
    "df_nuevos['CNN-3200-FIT32054.PV'] = rango * (len(df_nuevos) // len(rango)) + rango[:len(df_nuevos) % len(rango)]\n",
    "\n",
    "# Rellenar valores faltantes\n",
    "df_nuevos = df_nuevos.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "# Seleccionar sensores y calcular desviaciones est√°ndar\n",
    "columnas_sensores = [\n",
    "    'CNN-3200-CR_0001_MO.PWR',\n",
    "    'CNN-3200-CR_0001_MO.CUR',\n",
    "    'CNN-3200-FIT32053.PV',\n",
    "    'CNN-3200-FIT32054.PV',\n",
    "    'CNN-3200-PIT32031.PV',\n",
    "    'CNN-3200-PIT32043.PV',\n",
    "    'CNN-3200-PIT32056.PV',\n",
    "    'CNN-3200-TIT32045.PV',\n",
    "    'CNN-3200-TIT32046.PV'\n",
    "]\n",
    "\n",
    "for col in columnas_sensores:\n",
    "    df_nuevos[f\"{col}_std\"] = df_nuevos[col].rolling(window=4, min_periods=1).std()\n",
    "\n",
    "# Asegurar que las columnas sean las mismas que en el entrenamiento\n",
    "columnas_sensores_std = columnas_sensores + [f\"{col}_std\" for col in columnas_sensores]\n",
    "df_nuevos = df_nuevos[columnas_sensores_std]\n",
    "\n",
    "# üîπ Normalizar con el scaler guardado en el entrenamiento\n",
    "scaler = joblib.load(\"scaler.pkl\")  # Cargar el scaler usado en entrenamiento\n",
    "df_nuevos_scaled = scaler.transform(df_nuevos)  # Solo transformar, NO fit\n",
    "\n",
    "# üîπ Crear ventanas de tiempo (10 registros por cada predicci√≥n)\n",
    "secuencia = 10  # Mismo tama√±o usado en el modelo\n",
    "X_nuevos = np.array([df_nuevos_scaled[i - secuencia : i] for i in range(secuencia, len(df_nuevos))])\n",
    "\n",
    "# Verificar dimensiones\n",
    "print(f\"Shape de X_nuevos: {X_nuevos.shape}\")\n",
    "print(f\"Shape de X_train: {X_train.shape}\")\n",
    "\n",
    "# üîπ Cargar el modelo\n",
    "modelo = load_model(\"modelo_lstm_articulo.h5\")\n",
    "\n",
    "# üîπ Hacer predicciones\n",
    "y_nuevos_prob = modelo.predict(X_nuevos)\n",
    "\n",
    "# üîπ Aplicar umbral ajustado\n",
    "umbral = 0.3\n",
    "y_nuevos_pred = (y_nuevos_prob > umbral).astype(int)\n",
    "\n",
    "print(\"‚úÖ Predicciones completadas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribuci√≥n de predicciones:\")\n",
    "unique, counts = np.unique(y_nuevos_pred, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(y_nuevos_prob, bins=50, kde=True)\n",
    "plt.xlabel(\"Probabilidad de Falla\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.title(\"Distribuci√≥n de Probabilidades de Falla\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_umbral = 0.7\n",
    "y_nuevos_pred_ajustado = (y_nuevos_prob > nuevo_umbral).astype(int)\n",
    "\n",
    "unique, counts = np.unique(y_nuevos_pred_ajustado, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_nuevos_pred, y_nuevos_pred_ajustado)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Falla\"], yticklabels=[\"Normal\", \"Falla\"])\n",
    "plt.xlabel(\"Predicci√≥n Ajustada\")\n",
    "plt.ylabel(\"Predicci√≥n Original\")\n",
    "plt.title(f\"Matriz de Confusi√≥n - Umbral Ajustado a {nuevo_umbral}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
